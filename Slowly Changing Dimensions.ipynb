{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52c73c2",
   "metadata": {},
   "source": [
    "# Slowly Changing dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96b55a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slowly changing dimension defines the update strategy on the data. \n",
    "# The easiest way to ensure this is by making in-place changes, a little bit like with SQL updates, \n",
    "# UPDATE table SET a = \"1\", b = \"2\" WHERE id = 30. The problem with these queries is that,\n",
    "# despite their simplicity, they decrease the security of your data. \n",
    "# Let's imagine how to \"rollback\" the change and apply different business logic, \n",
    "# maybe because we misunderstood the former one. With in-place changes and\n",
    "# no raw data to regenerate the dataset from scratch, it can be hard.\n",
    "# That's where slowly changing dimension types came in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2313a522",
   "metadata": {},
   "source": [
    "# type 0 - no specific action performed.\n",
    "# type 1 - data is overwritten in-place, like with my UPDATE...SET example\n",
    "# type 2 - uses a concept of active/inactive row. When new information is present for a row, \n",
    "# it's used to build a completely new row flagged as \"active\". Previous \"active\" row passes to \"inactive\" state. In addition to the state transition, the columns with dates storing the validity period are updated accordingly.\n",
    "# type 3 - the table has one extra column per updatable field. \n",
    "# The currently used values are stored in \"current_\"-prefixed columns whereas previously used in \"previous_\"-prefixed columns. The history is then limited by the number of \"previous\" columns.\n",
    "# type 4 - the table looks like an append-only log file system. \n",
    "# Every change is added at the end of the table. In addition to that, every new row for given key changes the validity period of the previously active row.\n",
    "# type 6 - the combination of types 1, 2 and 3 (1+2+3 = 6), so you will retrieve a table with: \"previous\"-prefixed column, validity period and active/inactive flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a87b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "563d13c1",
   "metadata": {},
   "source": [
    "# Implementing Type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6d3cd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nemp_id,emp_name,emp_city,emp_salary\\n2, Peter, Melbourne, 55000.00\\n5, Jessie, Brisbane, 42000.00\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datasets we have \n",
    "\n",
    "# employee.csv\n",
    "\n",
    "\"\"\"\n",
    "emp_id,emp_name,emp_city,emp_salary\n",
    "1, John, Sydney, 35000.00\n",
    "2, Peter, Melbourne, 45000.00\n",
    "3, Sam, Sydney,55000.00\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# employee_delta.csv\n",
    "\n",
    "\"\"\"\n",
    "emp_id,emp_name,emp_city,emp_salary\n",
    "2, Peter, Melbourne, 55000.00\n",
    "5, Jessie, Brisbane, 42000.00\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54de8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective\n",
    "\n",
    "# 1) UPDATE record where emp_id=2 with the new salary info in the employee_delta.csv”.\n",
    "\n",
    "# 2) INSERT records that are new in the employee_delta.csv”.\n",
    "\n",
    "# NOTE: We don’t have to do DELETE as it is normally done as a logical delete with a new field “active=y” or “active=n”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cc3238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 \n",
    "\n",
    "# Initialize pyspark\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark3'\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_231'\n",
    "os.environ['HADOOP_HOME'] = 'C:\\spark3'\n",
    "spark_python = os.path.join(os.environ.get('SPARK_HOME',None),'python')\n",
    "py4j = glob.glob(os.path.join(spark_python,'lib','py4j-*.zip'))[0]\n",
    "sys.path[:0]=[spark_python,py4j]\n",
    "os.environ['PYTHONPATH']=py4j\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SCD\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37dd91ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+------+--------+---------+----------+\n",
      "|emp_id|emp_name| emp_city|emp_salary|emp_id|emp_name| emp_city|emp_salary|\n",
      "+------+--------+---------+----------+------+--------+---------+----------+\n",
      "|     2|   Peter|Melbourne|     45000|     2|   Peter|Melbourne|     55000|\n",
      "+------+--------+---------+----------+------+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2 : Inner join two dataframes to find the “emp_id” that is in both employee.csv & employee_delta.csv.\n",
    "employees_df = spark.read.csv(\"C:/Users/ShoryaSharma/Desktop/employee.csv\", header=\"true\", inferSchema=\"true\")\n",
    "employees_delta_df = spark.read.csv(\"C:/Users/ShoryaSharma/Desktop/employee_delta.csv\", header=\"true\", inferSchema=\"true\")\n",
    " \n",
    "#IDENTIFY RECORDS THAT ARE IN BOTH WITH AN \"INNER JOIN\"\n",
    " \n",
    "emp_updated = employees_df.join(employees_delta_df, employees_df.emp_id == employees_delta_df.emp_id, 'inner' )\n",
    "emp_updated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b873bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+\n",
      "|emp_id|emp_name| emp_city|emp_salary|\n",
      "+------+--------+---------+----------+\n",
      "|     2|   Peter|Melbourne|     55000|\n",
      "+------+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 3: Let’s SELECT the column values from employee_delta.csv as it will update the values in employee.csv.\n",
    "\n",
    "#IDENTIFY RECORDS THAT ARE IN BOTH WITH AN \"INNER JOIN\"\n",
    " \n",
    "emp_updated = emp_updated.select(employees_delta_df.emp_id, \n",
    "                                 employees_delta_df.emp_name, \n",
    "                                 employees_delta_df.emp_city, \n",
    "                                 employees_delta_df.emp_salary)\n",
    " \n",
    "emp_updated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b726d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+----------+\n",
      "|emp_id|emp_name|emp_city|emp_salary|\n",
      "+------+--------+--------+----------+\n",
      "|     1|    John|  Sydney|     35000|\n",
      "|     3|     Sam|  Sydney|     55000|\n",
      "+------+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 4: do a “left outer join”. We need to filter out records that are not in “employee_delta.csv”.\n",
    "\n",
    "emp_no_change_df = employees_df.join(employees_delta_df, employees_df.emp_id == employees_delta_df.emp_id, 'leftouter')\\\n",
    "  .filter(employees_delta_df.emp_id.isNull()) \\\n",
    "  .select(employees_df.emp_id, employees_df.emp_name, employees_df.emp_city, employees_df.emp_salary)\n",
    " \n",
    "emp_no_change_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f2041c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+----------+\n",
      "|emp_id|emp_name|emp_city|emp_salary|\n",
      "+------+--------+--------+----------+\n",
      "|     5|  Jessie|Brisbane|     42000|\n",
      "+------+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 5: do a “right outer join”. We need to filter out records that are in “employee.csv”.\n",
    "\n",
    "emp_new_df = employees_df.join(employees_delta_df, employees_df.emp_id == employees_delta_df.emp_id, 'rightouter')\\\n",
    "  .filter(employees_df.emp_id.isNull()) \\\n",
    "  .select(employees_delta_df.emp_id, employees_delta_df.emp_name, employees_delta_df.emp_city, employees_delta_df.emp_salary)\n",
    " \n",
    "emp_new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "057e90ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+\n",
      "|emp_id|emp_name| emp_city|emp_salary|\n",
      "+------+--------+---------+----------+\n",
      "|     1|    John|   Sydney|     35000|\n",
      "|     2|   Peter|Melbourne|     55000|\n",
      "|     3|     Sam|   Sydney|     55000|\n",
      "|     5|  Jessie| Brisbane|     42000|\n",
      "+------+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 6: Union all three dataframes – emp_updated,emp_no_change_df, and emp_new_df to give us the final values.\n",
    "\n",
    "emp_final = emp_updated.unionAll(emp_no_change_df).unionAll(emp_new_df).orderBy('emp_id')\n",
    " \n",
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7de71ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_final.coalesce(1).write.save(path='C:/Users/ShoryaSharma/Documents/employee_temp.csv', format='csv', header=\"true\", mode='overwrite', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4959de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_final = spark.read.csv('C:/Users/ShoryaSharma/Documents/employee_temp.csv', header=\"true\", inferSchema=\"true\")\n",
    "emp_final.coalesce(1).write.save(path='C:/Users/ShoryaSharma/Desktop/employee.csv', header=\"true\", format='csv', mode='overwrite', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e351da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+----------+\n",
      "|emp_id|emp_name| emp_city|emp_salary|\n",
      "+------+--------+---------+----------+\n",
      "|     1|    John|   Sydney|     35000|\n",
      "|     2|   Peter|Melbourne|     55000|\n",
      "|     3|     Sam|   Sydney|     55000|\n",
      "|     5|  Jessie| Brisbane|     42000|\n",
      "+------+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"C:/Users/ShoryaSharma/Desktop/employee.csv\", header=\"true\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e2b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
